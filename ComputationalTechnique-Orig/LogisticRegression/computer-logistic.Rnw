\SweaveOpts{keep.source=TRUE,pdf=TRUE,eps=TRUE,prefix.string=Figures/traditional}

<<setup,echo=F>>=
source('../../../setup.R')
require(ggplot2)
require(xtable)
@ 
 
Fitting logistic models uses many of the same ideas as in linear
models.

\subsection{Fitting Logistic Models}

\index{P}{glm@\texttt{glm}}
\index{P}{Modeling!glm@\texttt{glm}}
\index{C}{logistic regression!fitting}
The \code{glm} operator fits logistic models.  (It also fits other
kinds of models, but that's another story.)  \code{glm} takes model
design and data arguments that are identical to their counterparts in
\code{lm}.  Here's an example using the smoking/mortality data: \datasetWhickham
<<>>=
whickham = fetchData("whickham.csv")
mod = glm( outcome ~ age + smoker, data=whickham, 
    family="binomial")
@ 
The last argument, \texttt{family="binomial"}, simply specifies to
\function{glm} that the logistic transformation should be used.
\function{glm} is short for Generalized Linear Modeling, a broad label
that covers logistic regression as well as other types of models
involving links and transformations.

\index{P}{summary@\texttt{summary}!for logistic regression}
\index{P}{Modeling!summary@\texttt{summary}!for logistic regression}
\index{P}{Hypothesis Testing!summary@\texttt{summary}!for logistic regression}
The regression report is produced with the \code{summary} operator,
which recognizes that the model was fit logistically and does the
right thing:
<<results=hide>>=
summary(mod)
@ 

<<echo=false,results=tex>>=
print(xtable(summary(mod)), floating=FALSE)
@ 


Keep in mind that the coefficients refer to the intermediate model
values $Y$.  The probability $P$ will be $e^Y / (1 + e^Y )$.


\subsection{Fitted Model Values}

\index{P}{predict@\texttt{predict}!in logistic regression}
\index{P}{fitted@\texttt{fitted}!in logistic regression}
\index{P}{Modeling!predict@\texttt{predict}!in logistic regression}
\index{P}{Modeling!fitted@\texttt{fitted}!in logistic regression}
\index{C}{link value!in logistic regression}
\index{C}{probability values!in logistic regression}

Logistic regression involves two different kinds of fitted values:
the intermediate ``link'' value $Y$ and the probability $P$.  The
\code{fitted} operator returns the probabilities:
<<>>=
probs = fitted(mod) 
@ 
There is one fitted probability value for each case.

The link values can be gotten via the \code{predict} operator
<<results=hide>>=
predict(mod, type="link")
@ 
<<echo=false>>=
brief.output(predict(mod, type="link"),14)
@ 

Notice that the link values are not necessarily between zero and one.

The \code{predict} operator can also be used to calculate the
probability values.
<<results=hide>>=
predict(mod, type="response")
@ 
<<echo=false>>=
brief.output(predict(mod, type="response"),14)
@ 
 
\noindent This is particularly useful when you want to use \code{predict} to
find the model values for inputs other than that original data frame
used for fitting.

\subsection{Which Level is ``Yes''?}

In fitting a logistic model, it's crucial that the response variable
be categorical, with two levels.  It happens that in the
\code{whickham} data, the \VN{outcome} variable fits the bill: the
levels are \code{Alive} and \code{Dead}.

The \code{glm} software will automatically recode the response
variable as 0/1.  The question is, which level gets mapped to 1?  In
some sense, it makes no difference since there are only two levels.
But if you're talking about the probability of dying, it's nice not
to mistake that for the probability of staying alive.  So make sure
that you know which level in the response variable corresponds to 1:
it's the second level.  

Here is an easy way to make sure which level has been coded as ``Yes''.
First, fit the all-cases-the-same model, \model{\VN{outcome}}{1}.  The
fitted model value $P$ from this model will be the proportion of cases for
which the outcome was ``Yes.''
<<>>=
mod2 = glm( outcome ~ 1, data=whickham, family="binomial")
@ 
<<eval=false,results=hide>>=
fitted(mod2)
@ 
<<echo=false>>=
brief.output(fitted(mod2),nstart=5)
@ 
So, 28\% of the cases were ``Yes.''  But which of the two levels is ``Yes?''
Find out just by looking at the proportion of each level:
<<>>=
props(outcome, data=whickham)
@ 

If you want to dictate which of the two levels is going to be encoded
as 1, you can use a comparison operation to do so:
<<results=hide>>=
mod3 = glm( outcome=="Alive" ~ 1, data=whickham, 
   family="binomial")
@ 

<<echo=false,results=verbatim>>=
brief.output(fitted(mod3),nstart=5)
@ 

In this model, ``Yes'' means \code{Alive}.

\subsection{Analysis of Deviance}

The same basic logic used in analysis of variance applies to logistic
regression, although the quantity being broken down into parts is not
the sum of squares of the residuals but, rather, the \newword{deviance}.

The \code{anova} software will take apart a logistic model, term by
term, using the order specified in the model.
\index{P}{anova@\texttt{anova}!in logistic regression}
<<results=hide>>=
anova(mod, test="Chisq")
@ 

<<echo=false,results=tex>>=
print(xtable(anova(mod, test="Chisq")), floating=FALSE)
@ 


\noindent Notice the second argument, \code{test="Chisq"}, which instructs
\code{anova} to calculate a p-value for each term.  This involves a
slightly different test than the F test used in linear-model \ANOVA.

The format of the \ANOVA\ table for logistic regression is somewhat
different from that used in linear models, but the concepts of degrees
of freedom and partitioning still apply.  The basic idea is to ask
whether the reduction in deviance accomplished with the model terms is
greater than what would be expected if random terms were used instead.
